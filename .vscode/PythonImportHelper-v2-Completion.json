[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "faiss",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "faiss",
        "description": "faiss",
        "detail": "faiss",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "importPath": "gentrain.distance_matrix",
        "description": "gentrain.distance_matrix",
        "isExtraImport": true,
        "detail": "gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "importPath": "gentrain.distance_matrix",
        "description": "gentrain.distance_matrix",
        "isExtraImport": true,
        "detail": "gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "importPath": "gentrain.distance_matrix",
        "description": "gentrain.distance_matrix",
        "isExtraImport": true,
        "detail": "gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "hnswlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hnswlib",
        "description": "hnswlib",
        "detail": "hnswlib",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "combinations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "combinations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "combinations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Align",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "Align",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "Align",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "SeqIO",
        "importPath": "Bio",
        "description": "Bio",
        "isExtraImport": true,
        "detail": "Bio",
        "documentation": {}
    },
    {
        "label": "kendalltau",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "kendalltau",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "kendalltau",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "importPath": "gentrain.nextclade",
        "description": "gentrain.nextclade",
        "isExtraImport": true,
        "detail": "gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "importPath": "gentrain.nextclade",
        "description": "gentrain.nextclade",
        "isExtraImport": true,
        "detail": "gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "importPath": "gentrain.nextclade",
        "description": "gentrain.nextclade",
        "isExtraImport": true,
        "detail": "gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "pkgutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkgutil",
        "description": "pkgutil",
        "detail": "pkgutil",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "StringIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "MultiLabelBinarizer",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MultiLabelBinarizer",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "MultiLabelBinarizer",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "plotly.graph_objects",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.graph_objects",
        "description": "plotly.graph_objects",
        "detail": "plotly.graph_objects",
        "documentation": {}
    },
    {
        "label": "plotly.colors",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "plotly.colors",
        "description": "plotly.colors",
        "detail": "plotly.colors",
        "documentation": {}
    },
    {
        "label": "adjusted_rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "adjusted_rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "adjusted_rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "rand_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "importPath": "gentrain.graph",
        "description": "gentrain.graph",
        "isExtraImport": true,
        "detail": "gentrain.graph",
        "documentation": {}
    },
    {
        "label": "community",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "community",
        "description": "community",
        "detail": "community",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "flex_and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def flex_and_or_lsh(encodings, limit):\n    start = time.time()\n    candidates = {index: [] for index in encodings}\n    vector_length = len(encodings[0])\n    candidate_tuples = set()\n    hash_substractor = int(vector_length * (1/8))\n    hash_length = vector_length - hash_substractor\n    learning_rate_threshold = 0.1\n    learning_rate = 1.0\n    while len(candidate_tuples) < limit:",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_lsh_hash",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def get_lsh_hash(binary_vector, random_indices):\n    sampled_bits = [binary_vector[i] for i in random_indices]\n    hash_value = \"\".join(map(str, sampled_bits))\n    return hash_value\ndef add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "add_to_lsh_table",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)\n    return lsh_table\ndef and_or_lsh(encodings, hash_length, iterations):\n    start = time.time()\n    candidates = defaultdict(set)",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def and_or_lsh(encodings, hash_length, iterations):\n    start = time.time()\n    candidates = defaultdict(set)\n    vector_length = len(next(iter(encodings.values())))\n    for iteration in range(iterations):\n        hash_function = random.sample(range(vector_length), hash_length)\n        lsh_table = defaultdict(list)\n        for index, encoding in encodings.items():\n            sampled_bits = [encoding[i] for i in hash_function]\n            hash_value = tuple(sampled_bits) ",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "faiss_k_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def faiss_k_candidates(vectors_dict, k, index):\n    candidates = {}\n    for vector_id, vector in vectors_dict.items():\n        _, faiss_candidates = index.search(np.array([vector]), k + 1)\n        faiss_candidates = faiss_candidates[0]\n        faiss_candidates = [\n            candidate_id\n            for candidate_id in faiss_candidates\n            if candidate_id != vector_id\n        ]",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "k_faiss_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def k_faiss_hnsw_candidates(vectors_dict, k_nearest_neighbors):\n    faiss_candidates = {}\n    index = faiss.IndexHNSWBinaryB(64, 32)\n    index.train(np.array(list(vectors_dict.values()), dtype=np.float32))\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    for k in k_nearest_neighbors:\n        faiss_candidates[k] = faiss_k_candidates(vectors_dict, k, index)\n    return faiss_candidates\ndef faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "faiss_cluster_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}\n    fallback_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(\n            np.array([vector]), len(vectors_dict)\n        )\n        filtered_distances = []\n        filtered_candidates = []\n        for candidate_index, candidate in enumerate(faiss_candidates[0]):",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def get_hnsw_candidates(\n    encodings, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    dim = 128\n    num_elements = len(encodings)\n    data = np.float32(np.array(list(encodings.values())))\n    ids = np.arange(len(data))\n    index = hnswlib.Index(space = 'l2', dim = len(data[0]))\n    index.init_index(max_elements = num_elements, ef_construction = 200, M = 16)",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "breadth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def breadth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_matrix = get_bitwise_xor_distance_matrix(vectors_dict)\n    distance_collection_start = time.time()\n    for vector_id, candidate_distances in enumerate(distance_matrix):\n        for candidate_id, distance in enumerate(candidate_distances):\n            if candidate_id == vector_id:\n                continue\n            candidate_tuples_with_distances[",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "depth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def depth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_collection_start = time.time()\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    for vector_id, encoding in vectors_dict.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[vector_id])\n        xor_distances = np.unpackbits(xor_result, axis=1).sum(axis=1)\n        for candidate_id, xor_distance in enumerate(xor_distances):\n            if candidate_id == vector_id:",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.candidate_sourcing",
        "description": "gentrain.build.lib.gentrain.candidate_sourcing",
        "peekOfCode": "def bitwise_xor_candidates(\n    vectors_dict, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    if search_method == \"breadth\":\n        candidates = breadth_bitwise_xor_candidates(vectors_dict, limit, packed_vectors)\n    else:",
        "detail": "gentrain.build.lib.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_data_aggregate_sample",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.dataset",
        "description": "gentrain.build.lib.gentrain.dataset",
        "peekOfCode": "def get_data_aggregate_sample(dataframe, size, seasonal_column, aggregate):\n    aggregate_path = f\"aggregates/{aggregate}\"\n    if not os.path.exists(aggregate_path):\n        os.mkdir(aggregate_path)\n    if not os.path.exists(f\"{aggregate_path}/{size}\"):\n        os.mkdir(f\"{aggregate_path}/{size}\")\n    if os.path.exists(f\"{aggregate_path}/sequences_and_metadata.csv\"):\n        shutil.rmtree(f\"{aggregate_path}/sequences_and_metadata.csv\")\n    n_per_interval = int(size/len(dataframe[seasonal_column].value_counts()))\n    sample = dataframe.groupby(seasonal_column).apply(lambda x: x.sample(n=min(n_per_interval, len(x)), random_state=42), include_groups=False)",
        "detail": "gentrain.build.lib.gentrain.dataset",
        "documentation": {}
    },
    {
        "label": "prepend_manual",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "def prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "def get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        if position not in mutation_positions:\n            mutation_positions[position] = {}\n        mutation_positions[position][\"snp\"] = character\n    for insertion in sequence_mutations[\"insertions\"]:",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "align_samples",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "def align_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1 = np.array([], dtype=str)\n    sequence_2 = np.array([], dtype=str)\n    for current_base_index in range(len(reference_sequence_array) - 1):\n        reference_index_char = reference_sequence_array[current_base_index]\n        additions_1 = np.array([], dtype=str)\n        additions_2 = np.array([], dtype=str)\n        # add remaining reference characters if index is not in positions dictionary\n        if current_base_index not in mutation_positions_1:\n            additions_1 = np.append(additions_1, reference_index_char)",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "calculate_distance",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "def calculate_distance(sequence_1, sequence_2):\n    distance = 0\n    proper_threshold = 5\n    proper_chars_1 = 0\n    proper_chars_2 = 0\n    n_count_1 = np.count_nonzero(sequence_1 == \"N\")\n    n_count_2 = np.count_nonzero(sequence_2 == \"N\")\n    sequence_length_1 = sequence_1.size\n    sequence_length_2 = sequence_2.size\n    active_gap_1 = False",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_distance_for_two_samples",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "def get_distance_for_two_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1, sequence_2 = align_samples(mutation_positions_1, mutation_positions_2)\n    return calculate_distance(sequence_1, sequence_2)",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "ambiguous_chars",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "ambiguous_chars = {\n    \"A\": [\"A\"],\n    \"C\": [\"C\"],\n    \"G\": [\"G\"],\n    \"T\": [\"T\"],\n    \"U\": [\"U\"],\n    \"M\": [\"A\", \"C\"],\n    \"R\": [\"A\", \"G\"],\n    \"S\": [\"C\", \"G\"],\n    \"W\": [\"A\", \"T\"],",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "reference_sequence = \"\"\nfor record in SeqIO.parse((\"../gentrain/gentrain/reference.fasta\"), \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\naligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "aligner",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.distance_calculation",
        "description": "gentrain.build.lib.gentrain.distance_calculation",
        "peekOfCode": "aligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:",
        "detail": "gentrain.build.lib.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_bitwise_xor_distance_matrix(encodings):\n    start = time.time()\n    hamming_distance_matrix = []\n    packed_vectors = np.packbits(np.array(list(encodings.values())), axis=1)\n    for isolate_id, isolate_encoding in encodings.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[isolate_id])\n        hamming_distance_matrix.append(np.unpackbits(xor_result, axis=1).sum(axis=1))\n    hamming_distance_matrix = np.array(hamming_distance_matrix)\n    end = time.time()\n    print(f\"matrix generation time: {round(end - start, 2)}s\")",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_kendall_tau_correlation",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_kendall_tau_correlation(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    correlation, p = kendalltau(matrix_1, matrix_2)\n    return correlation\ndef get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_signed_rmse",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse\ndef get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_signed_infection_rmse",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    matrix_1 = matrix_1[infection_mask_1]\n    matrix_2 = matrix_2[infection_mask_1]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_recall",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_recall(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fn = np.sum(infection_mask_1 & not_infection_mask_2)\n    return tp/(tp+fn)",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_precision",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_precision(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fp = np.sum(not_infection_mask_1 & infection_mask_2)\n    return tp/(tp+fp)",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_f1",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_f1(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    tn = np.sum(not_infection_mask_1 & not_infection_mask_2)",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "median_distance",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.distance_matrix",
        "description": "gentrain.build.lib.gentrain.distance_matrix",
        "peekOfCode": "def median_distance(matrix):\n    triu_mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n    matrix = matrix[triu_mask]\n    return np.median(matrix)",
        "detail": "gentrain.build.lib.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_missing_positions",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1\n        for position in range(start,end):  \n            if position >= mutations[\"alignmentStart\"] or position <= mutations[\"alignmentEnd\"]:\n                missing_positions.append(position)\n    return missing_positions",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "filter_mutations_by_missings",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def filter_mutations_by_missings(mutations, isolates_df, shifted_relevant_mutations, shifted_reference_positions):\n    mutations[\"alignmentStart\"] = isolates_df[\"alignmentStart\"]\n    mutations[\"alignmentEnd\"] = isolates_df[\"alignmentEnd\"]\n    missing_positions = mutations.apply(lambda x: get_missing_positions(x), axis=1)\n    all_missing_positions = list(itertools.chain.from_iterable(missing_positions))\n    missing_position_appearances = Counter(all_missing_positions)\n    filtered_positions = []\n    for position in range(len(reference_sequence_array)):\n        if missing_position_appearances[position] > (0.05*len(isolates_df)):\n            filtered_positions.append(position)",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "find_and_shift_relevant_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering):\n    # filter substitions by relevance based on observed behaviours in MST constellations\n    substitution_counts = mutations['substitutions'].explode().value_counts().reset_index()\n    if use_frequency_filtering:\n        substitution_counts = substitution_counts[(substitution_counts[\"count\"]/len(isolates_df) < 0.2) & (substitution_counts[\"count\"] > 1)]\n    relevant_substitution_positions = list(substitution_counts[\"substitutions\"].apply(lambda x: int(re.match(r'^([A-Z])(\\d+)([A-Z])$', x).group(2))))\n    shifted_relevant_substitution_positions = [shifted_reference_positions[position] for position in relevant_substitution_positions]\n    if exclude_indels:\n        relevant_substitution_positions.sort()\n        relevant_shifted_substitutions = minified_aligned_sequences[shifted_relevant_substitution_positions]",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_inserted_lengths",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_inserted_lengths(unique_insertion_list):\n    inserted_lengths = {}\n    for insertion in unique_insertion_list:\n        regex_result = re.findall(r'[0-9]+|:|[A-Za-z]+', insertion)\n        position = int(regex_result[0])\n        inserted_chars = regex_result[2]\n        if position not in inserted_lengths or position not in inserted_lengths and len(inserted_lengths[position]) < len(inserted_chars):\n            inserted_lengths[position] = 1\n    return dict(sorted(inserted_lengths.items()))\ndef get_shifted_reference_positions(unique_insertion_list):",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_shifted_reference_positions(unique_insertion_list):\n    shifted_reference_positions = {x:x for x in range(1,len(reference_sequence_array)+1)}\n    current_shift = 0\n    inserted_lengths = get_inserted_lengths(unique_insertion_list)\n    for position in shifted_reference_positions:\n        shifted_reference_positions[position] = shifted_reference_positions[position] + current_shift\n        # increment shift for the next position, because 3073:GAA inserts GAA between 3073 and 3074 so that 3074 turns into 3077\n        if position in inserted_lengths.keys():\n            current_shift += inserted_lengths[position]\n    return shifted_reference_positions",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_reference",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_aligned_reference(unique_insertion_list, shifted_reference_positions):\n    aligned_reference = {}\n    aligned_sequences_length = list(shifted_reference_positions.values())[-1]+1\n    for aligned_position in range(1, aligned_sequences_length+1):\n        if aligned_position in list(shifted_reference_positions.values()):\n            reference_position = list(shifted_reference_positions.keys())[list(shifted_reference_positions.values()).index(aligned_position)]\n            aligned_reference[aligned_position] = reference_sequence[reference_position - 1]\n        else:\n            aligned_reference[aligned_position] = \"-\"\n    return aligned_reference",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_mutation_dict",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = {i: 0 for i in range(1, len(aligned_reference)+1)}\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        aligned_sequence[shifted_reference_positions[position]] = 1\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:\n        match = re.match(r'^(\\d+):([A-Z]+)$', insertion)",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_nucleotide_dict",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = aligned_reference.copy()\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        aligned_sequence[shifted_reference_positions[position]] = character\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions_from_csv",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_shifted_reference_positions_from_csv(isolates_df):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    return shifted_reference_positions\ndef filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "filter_and_align_mutations",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)\n    aligned_sequences_dict = list(mutations.apply(lambda row: get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels) if mutation_sensitive else get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels), axis=1))\n    aligned_sequences_df = pd.DataFrame(aligned_sequences_dict)\n    minified_aligned_sequences = aligned_sequences_df\n    shifted_relevant_mutations, shifted_reference_positions = find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering)\n    if filter_N:",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "trim_vocab_for_faiss",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def trim_vocab_for_faiss(mutations, m):\n    columns_to_drop = len(mutations.columns) % m\n    cols = []\n    if columns_to_drop > 0:\n        for col in mutations.columns:\n            value_counts = mutations[col].value_counts(dropna=False)\n            if len(value_counts) == 2 and (1 in value_counts.values or 2 in value_counts.values):\n                cols.append(col)\n                columns_to_drop = columns_to_drop - 1\n            if columns_to_drop == 0:",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_mutation_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_mutation_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, True, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    end = time.time()",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_nucleotide_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def get_nucleotide_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, False, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    one_hot_encodings = generate_one_hot_encoding(encodings)    ",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "generate_one_hot_encoding",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "def generate_one_hot_encoding(encodings):\n    unique_values = sorted(set(val for encoding in encodings for val in encoding))\n    value_to_index = {val: idx for idx, val in enumerate(unique_values)}\n    def one_hot_encode(encoding, value_to_index):\n        num_classes = len(value_to_index)\n        return np.eye(num_classes, dtype=\"float32\")[[value_to_index[val] for val in encoding]]\n    return [one_hot_encode(encoding, value_to_index).flatten() for encoding in encodings]",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "reference_sequence = \"\"\nreference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.encoding",
        "description": "gentrain.build.lib.gentrain.encoding",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1",
        "detail": "gentrain.build.lib.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_infection_rate",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_infection_rate(distance_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    filtered = distance_matrix_df.where(tri_mask).to_numpy()\n    infections_count = (filtered <= distance_threshold).sum().sum()\n    total_distances_count = np.count_nonzero(~np.isnan(filtered))\n    return infections_count/total_distances_count\ndef get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_infection_chain_participation_rate",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)\n    distance_matrix_df = distance_matrix_df.where(mask_off_diagonal)\n    condition = (distance_matrix_df < distance_threshold).any(axis=1)\n    return condition.sum()/len(distance_matrix_df)\ndef get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_infection_detection_scores",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()\n    observed_matrix_df = pd.DataFrame(observed_matrix)\n    filtered_observed_matrix = observed_matrix_df.where(tri_mask).to_numpy()\n    tp_mask = (filtered_observed_matrix <= distance_threshold) & (filtered_complete_matrix <= distance_threshold)\n    tp = len(np.argwhere(tp_mask))\n    tn_mask = (filtered_observed_matrix > distance_threshold) & (filtered_complete_matrix > distance_threshold)\n    tn = len(np.argwhere(tn_mask))",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_lineage_purity",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_lineage_purity(lineages, communities):\n    df = pd.DataFrame({\n        \"community\": communities,\n        \"lineage\":   lineages\n    })\n    counts = df.groupby([\"community\", \"lineage\"]) \\\n               .size() \\\n               .rename(\"count\")\n    community_sizes = counts.groupby(level=0) \\\n                            .sum() \\",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "candidate_evaluation_and_matrices",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def candidate_evaluation_and_matrices(candidates, distance_matrix, runtime):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    filtered = distance_matrix_df.where(tri_mask)\n    infections_count = (filtered < 2).sum().sum()\n    total_distances_count = filtered.count().sum()\n    mask = np.zeros_like(distance_matrix, dtype=bool)\n    for i, candidates in candidates.items():\n        for j in candidates:\n            mask[i, j] = True",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_community_map",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_community_map(partition):\n    community_map = {}\n    for sample_id, community_id in partition.items():\n        if community_id not in community_map:\n            community_map[community_id] = []\n        community_map[community_id].append(sample_id)\n    return community_map\ndef get_communities_larger_than(community_map, min_size):\n    return {\n        k: v",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_communities_larger_than",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_communities_larger_than(community_map, min_size):\n    return {\n        k: v\n        for k, v in community_map.items()\n        if isinstance(v, list) and len(v) > min_size\n    }\ndef get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_community_labels_for_sample_ids",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "candidate_graph_evaluation",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels\n        ),\n        \"lineage_purity\": get_lineage_purity(lineages, candidate_community_labels),",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_candidate_evaluation_and_export_mst",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_candidate_evaluation_and_export_mst(\n    method_name,\n    candidates,\n    graph_path,\n    distance_matrix,\n    complete_community_labels,\n    complete_mst,\n    lineages,\n    isolates_df,\n    runtime,",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_computation_rate_plot",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.evaluation",
        "description": "gentrain.build.lib.gentrain.evaluation",
        "peekOfCode": "def get_computation_rate_plot(field, evaluation_collection, y_axis_title, legend=dict(\n            x=0.55,\n            y=0.1,\n            xanchor=\"left\",\n            yanchor=\"bottom\",\n            font=dict(size=35),\n        ), computation_rates=[0.05,0.1,0.15,0.2]):\n    fig = go.Figure()\n    for index, method_evaluation in enumerate(evaluation_collection.items()):\n        method = method_evaluation[0]",
        "detail": "gentrain.build.lib.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def get_outbreak_community_labels(graph, distance_threshold = None):\n    graph_copy = graph.copy()\n    if distance_threshold:\n        edges_to_remove = [(u, v) for u, v, d in graph_copy.edges(data=True) if d['weight'] > distance_threshold]\n        graph_copy.remove_edges_from(edges_to_remove)\n    partition = community_louvain.best_partition(graph_copy, random_state=42)\n    community_labels = list(partition.values())\n    return community_labels\ndef get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_connected_component_labels",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()\n    edges_to_remove = [(u, v) for u, v, d in mst_copy.edges(data=True) if d['weight'] > distance_threshold]\n    mst_copy.remove_edges_from(edges_to_remove)\n    communities = list(nx.connected_components(mst_copy))\n    outbreak_communities = sorted(communities, key=len, reverse=True)\n    community_labels = {}\n    for community_id, community_nodes in enumerate(outbreak_communities):\n        for node_id in community_nodes:\n            community_labels[node_id] = community_id",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def mean_edge_weight(G):\n    total_weight = sum(data.get(\"weight\") for u, v, data in G.edges(data=True))\n    num_edges = G.number_of_edges()\n    return total_weight / num_edges if num_edges > 0 else 0.0\ndef median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "median_edge_weight",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:\n            data[\"weight\"] = 0.1   \n    end = time.time()",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def build_graph(distance_matrix):\n    graph = nx.Graph()\n    n = distance_matrix.shape[0]\n    graph.add_nodes_from(range(n))\n    for i in range(n):\n        for j in range(i+1, n):\n            distance = distance_matrix[i][j]\n            if not np.isnan(distance):\n                label = \"regular\"\n                if distance < 2:",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.graph",
        "description": "gentrain.build.lib.gentrain.graph",
        "peekOfCode": "def export_graph_gexf(graph, community_labels, dataset, name):\n    datetime_sampling_dates = pd.to_datetime(dataset[\"date_of_sampling\"])\n    numeric_dates = (datetime_sampling_dates - datetime_sampling_dates.min()).dt.days\n    nx.set_node_attributes(graph, {node: community_labels[int(node)] for node in graph.nodes()}, name=\"community\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"clade\"] for node in graph.nodes()}, name=\"clade\")            \n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"Nextclade_pango\"] for node in graph.nodes()}, name=\"pango\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.city\"] for node in graph.nodes()}, name=\"city\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.state\"] for node in graph.nodes()}, name=\"state\")\n    nx.set_node_attributes(graph, {node: numeric_dates.iloc[int(node)] for node in graph.nodes()}, name=\"sampling_date\")\n    nx.write_gexf(graph, f\"{name}.gexf\")",
        "detail": "gentrain.build.lib.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "generate_graph",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "description": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "peekOfCode": "def generate_graph(distance_matrix):\n    graph = nx.Graph()\n    fasta_ids = list(distance_matrix.keys())\n    edges = []\n    for index_1, fasta_id_1 in enumerate(distance_matrix):\n        for fasta_id_2 in distance_matrix[fasta_id_1]:\n            edges.append((fasta_ids[index_1], distance_matrix[fasta_id_1][fasta_id_2], {\"weight\": distance_matrix[fasta_id_1][fasta_id_2]}))\n    graph.add_edges_from(\n        edges\n    )",
        "detail": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "documentation": {}
    },
    {
        "label": "generate_mst",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "description": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "peekOfCode": "def generate_mst(graph):\n    mst = nx.minimum_spanning_tree(graph)\n    return mst\ndef visualize_mst(mst):\n    plt.figure(3, figsize=(10, 10))\n    pos = nx.spring_layout(mst, k=0.5, iterations=200, weight=\"distance\")\n    nx.draw_networkx_nodes(mst, pos, node_color=\"lightblue\", node_size=100)\n    nx.draw_networkx_labels(mst, pos, font_size=10, font_family=\"sans-serif\")\n    nx.draw_networkx_edge_labels(\n        mst, pos, edge_labels={(u, v): d[\"weight\"] for u, v, d in mst.edges(data=True)}",
        "detail": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "documentation": {}
    },
    {
        "label": "visualize_mst",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "description": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "peekOfCode": "def visualize_mst(mst):\n    plt.figure(3, figsize=(10, 10))\n    pos = nx.spring_layout(mst, k=0.5, iterations=200, weight=\"distance\")\n    nx.draw_networkx_nodes(mst, pos, node_color=\"lightblue\", node_size=100)\n    nx.draw_networkx_labels(mst, pos, font_size=10, font_family=\"sans-serif\")\n    nx.draw_networkx_edge_labels(\n        mst, pos, edge_labels={(u, v): d[\"weight\"] for u, v, d in mst.edges(data=True)}\n    )\n    nx.draw_networkx_edges(mst, pos, edge_color=\"green\", width=1)\n    plt.axis(\"off\")",
        "detail": "gentrain.build.lib.gentrain.minimum_spanning_tree",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.nextclade",
        "description": "gentrain.build.lib.gentrain.nextclade",
        "peekOfCode": "def get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.build.lib.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_csv",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.nextclade",
        "description": "gentrain.build.lib.gentrain.nextclade",
        "peekOfCode": "def get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.build.lib.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.nextclade",
        "description": "gentrain.build.lib.gentrain.nextclade",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):",
        "detail": "gentrain.build.lib.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.build.lib.gentrain.nextclade",
        "description": "gentrain.build.lib.gentrain.nextclade",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)",
        "detail": "gentrain.build.lib.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "get_lsh_hash",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def get_lsh_hash(binary_vector, random_indices):\n    sampled_bits = [binary_vector[i] for i in random_indices]\n    hash_value = \"\".join(map(str, sampled_bits))\n    return hash_value\ndef add_to_lsh_table(lsh_table, vector_id, vector, random_indices):\n    bucket_id = get_lsh_hash(vector, random_indices)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "add_to_lsh_table",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def add_to_lsh_table(lsh_table, vector_id, vector, random_indices):\n    bucket_id = get_lsh_hash(vector, random_indices)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)\n    return lsh_table\ndef xor_lsh_dynamic(vectors_dict, forced_computation_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "xor_lsh_dynamic",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def xor_lsh_dynamic(vectors_dict, forced_computation_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()\n    total_distance_count = (len(vectors_dict) * (len(vectors_dict) - 1)) / 2\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    candidates = {vector_id: [] for vector_id in vectors_dict}\n    computation_rate = 0\n    iterations = 0\n    considered_distance_count = 0",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "xor_lsh_hash_length",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def xor_lsh_hash_length(vectors_dict, hash_length, min_learning_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()\n    total_distance_count = (len(vectors_dict) * (len(vectors_dict) - 1)) / 2\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    candidates = {vector_id: [] for vector_id in vectors_dict}\n    computation_rate = 0\n    considered_distance_count = 0\n    learning_rate_saturation = 0",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "xor_lsh_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def xor_lsh_candidates(vectors_dict, hash_lengths, min_learning_rates):\n    lsh_candidates = {}\n    runtime = {}\n    for hash_length in hash_lengths:\n        lsh_candidates[hash_length] = {}\n        runtime[hash_length] = {}\n        for min_learning_rate in min_learning_rates:\n            lsh_candidates[hash_length][min_learning_rate], runtime[hash_length][\"runtime\"] = xor_lsh(vectors_dict,\n                                                                                                      hash_length,\n                                                                                                      min_learning_rate)",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "get_k_nearest_neighbors",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def get_k_nearest_neighbors(vectors_dict, k):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    candidates = {}\n    for vector_id, vector in vectors_dict.items():\n        _, faiss_candidates = index.search(np.array([vector]), k)\n        candidates[vector_id] = faiss_candidates[0]",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "faiss_cluster_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}\n    fallback_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        filtered_distances = []\n        filtered_candidates = []\n        for candidate_index, candidate in enumerate(faiss_candidates[0]):\n            if cluster_labels[vector_id] != -1 and cluster_labels[vector_id] == cluster_labels[candidate]:\n                filtered_distances.append(faiss_distances[0][candidate_index])",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "faiss_depth_search",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def faiss_depth_search(vectors_dict, limit, index):\n    candidates = {}\n    distances = []\n    candidate_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        vector_distances = {}\n        for candidate_index, distance in enumerate(faiss_distances[0]):\n            candidate_id = faiss_candidates[0][candidate_index]\n            vector_distances[candidate_id] = distance",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "faiss_breadth_search",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def faiss_breadth_search(vectors_dict, limit, index):\n    candidates = {}\n    distances = []\n    candidate_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        vector_distances = {}\n        for candidate_index, distance in enumerate(faiss_distances[0]):\n            candidate_id = faiss_candidates[0][candidate_index]\n            vector_distances[candidate_id] = distance",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "k_faiss_exact_cluster_search_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def k_faiss_exact_cluster_search_candidates(vectors_dict, limit, cluster_labels, centroid_ids, print_execution_time=True):\n    start = time.time()\n    candidates = {}\n    for centroid_id in centroid_ids:\n        candidates[centroid_id] = [x for x in centroid_ids if x != centroid_id]\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    candidates = faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates)",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "faiss_l2_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def faiss_l2_candidates(vectors_dict, limit, search_method=\"depth\", print_execution_time=True):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    if search_method == \"breadth\":\n        candidates = faiss_breadth_search(encodings, limit, index)\n    else:\n        candidates = faiss_depth_search(encodings, limit, index)",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "faiss_hamming_candidates",
        "kind": 2,
        "importPath": "gentrain.build.lib.gentrain.xor",
        "description": "gentrain.build.lib.gentrain.xor",
        "peekOfCode": "def faiss_hamming_candidates(vectors_dict, limit, search_method=\"depth\", print_execution_time=True):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexBinaryFlat(vector_length*8)\n    index.add(np.array(list(vectors_dict.values())))\n    if search_method == \"breadth\":\n        candidates, distances = faiss_breadth_search(vectors_dict, limit, index)\n    else:\n        candidates, distances = faiss_depth_search(vectors_dict, limit, index)",
        "detail": "gentrain.build.lib.gentrain.xor",
        "documentation": {}
    },
    {
        "label": "get_lsh_hash",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def get_lsh_hash(binary_vector, random_indices):\n    sampled_bits = [binary_vector[i] for i in random_indices]\n    hash_value = \"\".join(map(str, sampled_bits))\n    return hash_value\ndef add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "add_to_lsh_table",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)\n    return lsh_table\ndef flex_and_or_lsh(encodings, limit):\n    start = time.time()\n    candidates = {index: [] for index in encodings}",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "flex_and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def flex_and_or_lsh(encodings, limit):\n    start = time.time()\n    candidates = {index: [] for index in encodings}\n    vector_length = len(encodings[0])\n    candidate_tuples = set()\n    hash_substractor = int(vector_length * (1/8))\n    hash_length = vector_length - hash_substractor\n    learning_rate_threshold = 0.1\n    learning_rate = 1.0\n    while len(candidate_tuples) < limit:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def and_or_lsh(encodings, hash_length, iterations):\n    start = time.time()\n    candidates = defaultdict(set)\n    vector_length = len(next(iter(encodings.values())))\n    for iteration in range(iterations):\n        hash_function = random.sample(range(vector_length), hash_length)\n        lsh_table = defaultdict(list)\n        for index, encoding in encodings.items():\n            sampled_bits = [encoding[i] for i in hash_function]\n            hash_value = tuple(sampled_bits) ",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_k_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def faiss_k_candidates(vectors_dict, k, index):\n    candidates = {}\n    for vector_id, vector in vectors_dict.items():\n        _, faiss_candidates = index.search(np.array([vector]), k + 1)\n        faiss_candidates = faiss_candidates[0]\n        faiss_candidates = [\n            candidate_id\n            for candidate_id in faiss_candidates\n            if candidate_id != vector_id\n        ]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "k_faiss_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def k_faiss_hnsw_candidates(vectors_dict, k_nearest_neighbors):\n    faiss_candidates = {}\n    index = faiss.IndexHNSWBinaryB(64, 32)\n    index.train(np.array(list(vectors_dict.values()), dtype=np.float32))\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    for k in k_nearest_neighbors:\n        faiss_candidates[k] = faiss_k_candidates(vectors_dict, k, index)\n    return faiss_candidates\ndef faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_cluster_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}\n    fallback_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(\n            np.array([vector]), len(vectors_dict)\n        )\n        filtered_distances = []\n        filtered_candidates = []\n        for candidate_index, candidate in enumerate(faiss_candidates[0]):",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def get_hnsw_candidates(\n    encodings, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    dim = 128\n    num_elements = len(encodings)\n    data = np.float32(np.array(list(encodings.values())))\n    ids = np.arange(len(data))\n    index = hnswlib.Index(space = 'l2', dim = len(data[0]))\n    index.init_index(max_elements = num_elements, ef_construction = 200, M = 16)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "breadth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def breadth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_matrix = get_bitwise_xor_distance_matrix(vectors_dict)\n    distance_collection_start = time.time()\n    for vector_id, candidate_distances in enumerate(distance_matrix):\n        for candidate_id, distance in enumerate(candidate_distances):\n            if candidate_id == vector_id:\n                continue\n            candidate_tuples_with_distances[",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "depth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def depth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_collection_start = time.time()\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    for vector_id, encoding in vectors_dict.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[vector_id])\n        xor_distances = np.unpackbits(xor_result, axis=1).sum(axis=1)\n        for candidate_id, xor_distance in enumerate(xor_distances):\n            if candidate_id == vector_id:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "peekOfCode": "def bitwise_xor_candidates(\n    vectors_dict, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    if search_method == \"breadth\":\n        candidates = breadth_bitwise_xor_candidates(vectors_dict, limit, packed_vectors)\n    else:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.candidate_sourcing-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_data_aggregate_sample",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.dataset-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.dataset-checkpoint",
        "peekOfCode": "def get_data_aggregate_sample(dataframe, size, seasonal_column, aggregate):\n    aggregate_path = f\"aggregates/{aggregate}\"\n    if not os.path.exists(aggregate_path):\n        os.mkdir(aggregate_path)\n    if not os.path.exists(f\"{aggregate_path}/{size}\"):\n        os.mkdir(f\"{aggregate_path}/{size}\")\n    if os.path.exists(f\"{aggregate_path}/sequences_and_metadata.csv\"):\n        shutil.rmtree(f\"{aggregate_path}/sequences_and_metadata.csv\")\n    n_per_interval = int(size/len(dataframe[seasonal_column].value_counts()))\n    sample = dataframe.groupby(seasonal_column).apply(lambda x: x.sample(n=min(n_per_interval, len(x)), random_state=42), include_groups=False)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.dataset-checkpoint",
        "documentation": {}
    },
    {
        "label": "prepend_manual",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "def prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "def get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        if position not in mutation_positions:\n            mutation_positions[position] = {}\n        mutation_positions[position][\"snp\"] = character\n    for insertion in sequence_mutations[\"insertions\"]:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "align_samples",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "def align_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1 = np.array([], dtype=str)\n    sequence_2 = np.array([], dtype=str)\n    for current_base_index in range(len(reference_sequence_array) - 1):\n        reference_index_char = reference_sequence_array[current_base_index]\n        additions_1 = np.array([], dtype=str)\n        additions_2 = np.array([], dtype=str)\n        # add remaining reference characters if index is not in positions dictionary\n        if current_base_index not in mutation_positions_1:\n            additions_1 = np.append(additions_1, reference_index_char)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "calculate_distance",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "def calculate_distance(sequence_1, sequence_2):\n    distance = 0\n    proper_threshold = 5\n    proper_chars_1 = 0\n    proper_chars_2 = 0\n    n_count_1 = np.count_nonzero(sequence_1 == \"N\")\n    n_count_2 = np.count_nonzero(sequence_2 == \"N\")\n    sequence_length_1 = sequence_1.size\n    sequence_length_2 = sequence_2.size\n    active_gap_1 = False",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_distance_for_two_samples",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "def get_distance_for_two_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1, sequence_2 = align_samples(mutation_positions_1, mutation_positions_2)\n    return calculate_distance(sequence_1, sequence_2)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "ambiguous_chars",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "ambiguous_chars = {\n    \"A\": [\"A\"],\n    \"C\": [\"C\"],\n    \"G\": [\"G\"],\n    \"T\": [\"T\"],\n    \"U\": [\"U\"],\n    \"M\": [\"A\", \"C\"],\n    \"R\": [\"A\", \"G\"],\n    \"S\": [\"C\", \"G\"],\n    \"W\": [\"A\", \"T\"],",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "reference_sequence = \"\"\nfor record in SeqIO.parse((\"../gentrain/gentrain/reference.fasta\"), \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\naligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "aligner",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "peekOfCode": "aligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_calculation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_bitwise_xor_distance_matrix(encodings):\n    start = time.time()\n    hamming_distance_matrix = []\n    packed_vectors = np.packbits(np.array(list(encodings.values())), axis=1)\n    for isolate_id, isolate_encoding in encodings.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[isolate_id])\n        hamming_distance_matrix.append(np.unpackbits(xor_result, axis=1).sum(axis=1))\n    hamming_distance_matrix = np.array(hamming_distance_matrix)\n    end = time.time()\n    print(f\"matrix generation time: {round(end - start, 2)}s\")",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_kendall_tau_correlation",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_kendall_tau_correlation(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    correlation, p = kendalltau(matrix_1, matrix_2)\n    return correlation\ndef get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_signed_rmse",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse\ndef get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_signed_infection_rmse",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    matrix_1 = matrix_1[infection_mask_1]\n    matrix_2 = matrix_2[infection_mask_1]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_recall",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_infection_recall(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fn = np.sum(infection_mask_1 & not_infection_mask_2)\n    return tp/(tp+fn)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_precision",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_infection_precision(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fp = np.sum(not_infection_mask_1 & infection_mask_2)\n    return tp/(tp+fp)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_f1",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def get_infection_f1(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    tn = np.sum(not_infection_mask_1 & not_infection_mask_2)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "median_distance",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "peekOfCode": "def median_distance(matrix):\n    triu_mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n    matrix = matrix[triu_mask]\n    return np.median(matrix)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.distance_matrix-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_missing_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1\n        for position in range(start,end):  \n            if position >= mutations[\"alignmentStart\"] or position <= mutations[\"alignmentEnd\"]:\n                missing_positions.append(position)\n    return missing_positions",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "filter_mutations_by_missings",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def filter_mutations_by_missings(mutations, isolates_df, shifted_relevant_mutations, shifted_reference_positions):\n    mutations[\"alignmentStart\"] = isolates_df[\"alignmentStart\"]\n    mutations[\"alignmentEnd\"] = isolates_df[\"alignmentEnd\"]\n    missing_positions = mutations.apply(lambda x: get_missing_positions(x), axis=1)\n    all_missing_positions = list(itertools.chain.from_iterable(missing_positions))\n    missing_position_appearances = Counter(all_missing_positions)\n    filtered_positions = []\n    for position in range(len(reference_sequence_array)):\n        if missing_position_appearances[position] > (0.05*len(isolates_df)):\n            filtered_positions.append(position)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "find_and_shift_relevant_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering):\n    # filter substitions by relevance based on observed behaviours in MST constellations\n    substitution_counts = mutations['substitutions'].explode().value_counts().reset_index()\n    if use_frequency_filtering:\n        substitution_counts = substitution_counts[(substitution_counts[\"count\"]/len(isolates_df) < 0.2) & (substitution_counts[\"count\"] > 1)]\n    relevant_substitution_positions = list(substitution_counts[\"substitutions\"].apply(lambda x: int(re.match(r'^([A-Z])(\\d+)([A-Z])$', x).group(2))))\n    shifted_relevant_substitution_positions = [shifted_reference_positions[position] for position in relevant_substitution_positions]\n    if exclude_indels:\n        relevant_substitution_positions.sort()\n        relevant_shifted_substitutions = minified_aligned_sequences[shifted_relevant_substitution_positions]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_inserted_lengths",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_inserted_lengths(unique_insertion_list):\n    inserted_lengths = {}\n    for insertion in unique_insertion_list:\n        regex_result = re.findall(r'[0-9]+|:|[A-Za-z]+', insertion)\n        position = int(regex_result[0])\n        inserted_chars = regex_result[2]\n        if position not in inserted_lengths or position not in inserted_lengths and len(inserted_lengths[position]) < len(inserted_chars):\n            inserted_lengths[position] = 1\n    return dict(sorted(inserted_lengths.items()))\ndef get_shifted_reference_positions(unique_insertion_list):",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_shifted_reference_positions(unique_insertion_list):\n    shifted_reference_positions = {x:x for x in range(1,len(reference_sequence_array)+1)}\n    current_shift = 0\n    inserted_lengths = get_inserted_lengths(unique_insertion_list)\n    for position in shifted_reference_positions:\n        shifted_reference_positions[position] = shifted_reference_positions[position] + current_shift\n        # increment shift for the next position, because 3073:GAA inserts GAA between 3073 and 3074 so that 3074 turns into 3077\n        if position in inserted_lengths.keys():\n            current_shift += inserted_lengths[position]\n    return shifted_reference_positions",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_aligned_reference",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_aligned_reference(unique_insertion_list, shifted_reference_positions):\n    aligned_reference = {}\n    aligned_sequences_length = list(shifted_reference_positions.values())[-1]+1\n    for aligned_position in range(1, aligned_sequences_length+1):\n        if aligned_position in list(shifted_reference_positions.values()):\n            reference_position = list(shifted_reference_positions.keys())[list(shifted_reference_positions.values()).index(aligned_position)]\n            aligned_reference[aligned_position] = reference_sequence[reference_position - 1]\n        else:\n            aligned_reference[aligned_position] = \"-\"\n    return aligned_reference",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_aligned_mutation_dict",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = {i: 0 for i in range(1, len(aligned_reference)+1)}\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        aligned_sequence[shifted_reference_positions[position]] = 1\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:\n        match = re.match(r'^(\\d+):([A-Z]+)$', insertion)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_aligned_nucleotide_dict",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = aligned_reference.copy()\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r'^([A-Z])(\\d+)([A-Z])$', substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        aligned_sequence[shifted_reference_positions[position]] = character\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions_from_csv",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_shifted_reference_positions_from_csv(isolates_df):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    return shifted_reference_positions\ndef filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "filter_and_align_mutations",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)\n    aligned_sequences_dict = list(mutations.apply(lambda row: get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels) if mutation_sensitive else get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels), axis=1))\n    aligned_sequences_df = pd.DataFrame(aligned_sequences_dict)\n    minified_aligned_sequences = aligned_sequences_df\n    shifted_relevant_mutations, shifted_reference_positions = find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering)\n    if filter_N:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "trim_vocab_for_faiss",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def trim_vocab_for_faiss(mutations, m):\n    columns_to_drop = len(mutations.columns) % m\n    cols = []\n    if columns_to_drop > 0:\n        for col in mutations.columns:\n            value_counts = mutations[col].value_counts(dropna=False)\n            if len(value_counts) == 2 and (1 in value_counts.values or 2 in value_counts.values):\n                cols.append(col)\n                columns_to_drop = columns_to_drop - 1\n            if columns_to_drop == 0:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_mutation_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_mutation_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, True, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    end = time.time()",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_nucleotide_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def get_nucleotide_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, False, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    one_hot_encodings = generate_one_hot_encoding(encodings)    ",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "generate_one_hot_encoding",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "def generate_one_hot_encoding(encodings):\n    unique_values = sorted(set(val for encoding in encodings for val in encoding))\n    value_to_index = {val: idx for idx, val in enumerate(unique_values)}\n    def one_hot_encode(encoding, value_to_index):\n        num_classes = len(value_to_index)\n        return np.eye(num_classes, dtype=\"float32\")[[value_to_index[val] for val in encoding]]\n    return [one_hot_encode(encoding, value_to_index).flatten() for encoding in encodings]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "reference_sequence = \"\"\nreference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r'^(\\d+)(?:-(\\d+))?$', missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1",
        "detail": "gentrain.gentrain..ipynb_checkpoints.encoding-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_rate",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_infection_rate(distance_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    filtered = distance_matrix_df.where(tri_mask).to_numpy()\n    infections_count = (filtered <= distance_threshold).sum().sum()\n    total_distances_count = np.count_nonzero(~np.isnan(filtered))\n    return infections_count/total_distances_count\ndef get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_chain_participation_rate",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)\n    distance_matrix_df = distance_matrix_df.where(mask_off_diagonal)\n    condition = (distance_matrix_df < distance_threshold).any(axis=1)\n    return condition.sum()/len(distance_matrix_df)\ndef get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_infection_detection_scores",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()\n    observed_matrix_df = pd.DataFrame(observed_matrix)\n    filtered_observed_matrix = observed_matrix_df.where(tri_mask).to_numpy()\n    tp_mask = (filtered_observed_matrix <= distance_threshold) & (filtered_complete_matrix <= distance_threshold)\n    tp = len(np.argwhere(tp_mask))\n    tn_mask = (filtered_observed_matrix > distance_threshold) & (filtered_complete_matrix > distance_threshold)\n    tn = len(np.argwhere(tn_mask))",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_lineage_purity",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_lineage_purity(lineages, communities):\n    df = pd.DataFrame({\n        \"community\": communities,\n        \"lineage\":   lineages\n    })\n    counts = df.groupby([\"community\", \"lineage\"]) \\\n               .size() \\\n               .rename(\"count\")\n    community_sizes = counts.groupby(level=0) \\\n                            .sum() \\",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "candidate_evaluation_and_matrices",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def candidate_evaluation_and_matrices(candidates, distance_matrix, runtime):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    filtered = distance_matrix_df.where(tri_mask)\n    infections_count = (filtered < 2).sum().sum()\n    total_distances_count = filtered.count().sum()\n    mask = np.zeros_like(distance_matrix, dtype=bool)\n    for i, candidates in candidates.items():\n        for j in candidates:\n            mask[i, j] = True",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_community_map",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_community_map(partition):\n    community_map = {}\n    for sample_id, community_id in partition.items():\n        if community_id not in community_map:\n            community_map[community_id] = []\n        community_map[community_id].append(sample_id)\n    return community_map\ndef get_communities_larger_than(community_map, min_size):\n    return {\n        k: v",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_communities_larger_than",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_communities_larger_than(community_map, min_size):\n    return {\n        k: v\n        for k, v in community_map.items()\n        if isinstance(v, list) and len(v) > min_size\n    }\ndef get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_community_labels_for_sample_ids",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "candidate_graph_evaluation",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels\n        ),\n        \"lineage_purity\": get_lineage_purity(lineages, candidate_community_labels),",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_candidate_evaluation_and_export_mst",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_candidate_evaluation_and_export_mst(\n    method_name,\n    candidates,\n    graph_path,\n    distance_matrix,\n    complete_community_labels,\n    complete_mst,\n    lineages,\n    isolates_df,\n    runtime,",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_computation_rate_plot",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "peekOfCode": "def get_computation_rate_plot(field, evaluation_collection, y_axis_title, legend=dict(\n            x=0.55,\n            y=0.1,\n            xanchor=\"left\",\n            yanchor=\"bottom\",\n            font=dict(size=35),\n        ), computation_rates=[0.05,0.1,0.15,0.2]):\n    fig = go.Figure()\n    for index, method_evaluation in enumerate(evaluation_collection.items()):\n        method = method_evaluation[0]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.evaluation-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def get_outbreak_community_labels(graph, distance_threshold = None):\n    graph_copy = graph.copy()\n    if distance_threshold:\n        edges_to_remove = [(u, v) for u, v, d in graph_copy.edges(data=True) if d['weight'] > distance_threshold]\n        graph_copy.remove_edges_from(edges_to_remove)\n    partition = community_louvain.best_partition(graph_copy, random_state=42)\n    community_labels = list(partition.values())\n    return community_labels\ndef get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_connected_component_labels",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()\n    edges_to_remove = [(u, v) for u, v, d in mst_copy.edges(data=True) if d['weight'] > distance_threshold]\n    mst_copy.remove_edges_from(edges_to_remove)\n    communities = list(nx.connected_components(mst_copy))\n    outbreak_communities = sorted(communities, key=len, reverse=True)\n    community_labels = {}\n    for community_id, community_nodes in enumerate(outbreak_communities):\n        for node_id in community_nodes:\n            community_labels[node_id] = community_id",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def mean_edge_weight(G):\n    total_weight = sum(data.get(\"weight\") for u, v, data in G.edges(data=True))\n    num_edges = G.number_of_edges()\n    return total_weight / num_edges if num_edges > 0 else 0.0\ndef median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "median_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:\n            data[\"weight\"] = 0.1   \n    end = time.time()",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def build_graph(distance_matrix):\n    graph = nx.Graph()\n    n = distance_matrix.shape[0]\n    graph.add_nodes_from(range(n))\n    for i in range(n):\n        for j in range(i+1, n):\n            distance = distance_matrix[i][j]\n            if not np.isnan(distance):\n                label = \"regular\"\n                if distance < 2:",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "peekOfCode": "def export_graph_gexf(graph, community_labels, dataset, name):\n    datetime_sampling_dates = pd.to_datetime(dataset[\"date_of_sampling\"])\n    numeric_dates = (datetime_sampling_dates - datetime_sampling_dates.min()).dt.days\n    nx.set_node_attributes(graph, {node: community_labels[int(node)] for node in graph.nodes()}, name=\"community\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"clade\"] for node in graph.nodes()}, name=\"clade\")            \n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"Nextclade_pango\"] for node in graph.nodes()}, name=\"pango\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.city\"] for node in graph.nodes()}, name=\"city\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.state\"] for node in graph.nodes()}, name=\"state\")\n    nx.set_node_attributes(graph, {node: numeric_dates.iloc[int(node)] for node in graph.nodes()}, name=\"sampling_date\")\n    nx.write_gexf(graph, f\"{name}.gexf\")",
        "detail": "gentrain.gentrain..ipynb_checkpoints.graph-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "peekOfCode": "def get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_csv",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "peekOfCode": "def get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):",
        "detail": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.nextclade-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_lsh_hash",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def get_lsh_hash(binary_vector, random_indices):\n    sampled_bits = [binary_vector[i] for i in random_indices]\n    hash_value = \"\".join(map(str, sampled_bits))\n    return hash_value\ndef add_to_lsh_table(lsh_table, vector_id, vector, random_indices):\n    bucket_id = get_lsh_hash(vector, random_indices)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "add_to_lsh_table",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def add_to_lsh_table(lsh_table, vector_id, vector, random_indices):\n    bucket_id = get_lsh_hash(vector, random_indices)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)\n    return lsh_table\ndef xor_lsh_dynamic(vectors_dict, forced_computation_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "xor_lsh_dynamic",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def xor_lsh_dynamic(vectors_dict, forced_computation_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()\n    total_distance_count = (len(vectors_dict) * (len(vectors_dict) - 1)) / 2\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    candidates = {vector_id: [] for vector_id in vectors_dict}\n    computation_rate = 0\n    iterations = 0\n    considered_distance_count = 0",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "xor_lsh_hash_length",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def xor_lsh_hash_length(vectors_dict, hash_length, min_learning_rate):\n    start = time.time()\n    vectors_dict = vectors_dict.copy()\n    total_distance_count = (len(vectors_dict) * (len(vectors_dict) - 1)) / 2\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    candidates = {vector_id: [] for vector_id in vectors_dict}\n    computation_rate = 0\n    considered_distance_count = 0\n    learning_rate_saturation = 0",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "xor_lsh_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def xor_lsh_candidates(vectors_dict, hash_lengths, min_learning_rates):\n    lsh_candidates = {}\n    runtime = {}\n    for hash_length in hash_lengths:\n        lsh_candidates[hash_length] = {}\n        runtime[hash_length] = {}\n        for min_learning_rate in min_learning_rates:\n            lsh_candidates[hash_length][min_learning_rate], runtime[hash_length][\"runtime\"] = xor_lsh(vectors_dict,\n                                                                                                      hash_length,\n                                                                                                      min_learning_rate)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "get_k_nearest_neighbors",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def get_k_nearest_neighbors(vectors_dict, k):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    candidates = {}\n    for vector_id, vector in vectors_dict.items():\n        _, faiss_candidates = index.search(np.array([vector]), k)\n        candidates[vector_id] = faiss_candidates[0]",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_cluster_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}\n    fallback_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        filtered_distances = []\n        filtered_candidates = []\n        for candidate_index, candidate in enumerate(faiss_candidates[0]):\n            if cluster_labels[vector_id] != -1 and cluster_labels[vector_id] == cluster_labels[candidate]:\n                filtered_distances.append(faiss_distances[0][candidate_index])",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_depth_search",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def faiss_depth_search(vectors_dict, limit, index):\n    candidates = {}\n    distances = []\n    candidate_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        vector_distances = {}\n        for candidate_index, distance in enumerate(faiss_distances[0]):\n            candidate_id = faiss_candidates[0][candidate_index]\n            vector_distances[candidate_id] = distance",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_breadth_search",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def faiss_breadth_search(vectors_dict, limit, index):\n    candidates = {}\n    distances = []\n    candidate_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(np.array([vector]), len(vectors_dict))\n        vector_distances = {}\n        for candidate_index, distance in enumerate(faiss_distances[0]):\n            candidate_id = faiss_candidates[0][candidate_index]\n            vector_distances[candidate_id] = distance",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "k_faiss_exact_cluster_search_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def k_faiss_exact_cluster_search_candidates(vectors_dict, limit, cluster_labels, centroid_ids, print_execution_time=True):\n    start = time.time()\n    candidates = {}\n    for centroid_id in centroid_ids:\n        candidates[centroid_id] = [x for x in centroid_ids if x != centroid_id]\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    candidates = faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_l2_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def faiss_l2_candidates(vectors_dict, limit, search_method=\"depth\", print_execution_time=True):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexFlatL2(vector_length)\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    if search_method == \"breadth\":\n        candidates = faiss_breadth_search(encodings, limit, index)\n    else:\n        candidates = faiss_depth_search(encodings, limit, index)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "faiss_hamming_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "description": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "peekOfCode": "def faiss_hamming_candidates(vectors_dict, limit, search_method=\"depth\", print_execution_time=True):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    index = faiss.IndexBinaryFlat(vector_length*8)\n    index.add(np.array(list(vectors_dict.values())))\n    if search_method == \"breadth\":\n        candidates, distances = faiss_breadth_search(vectors_dict, limit, index)\n    else:\n        candidates, distances = faiss_depth_search(vectors_dict, limit, index)",
        "detail": "gentrain.gentrain..ipynb_checkpoints.xor-checkpoint",
        "documentation": {}
    },
    {
        "label": "flex_and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def flex_and_or_lsh(encodings, limit):\n    start = time.time()\n    candidates = {index: [] for index in encodings}\n    vector_length = len(encodings[0])\n    candidate_tuples = set()\n    hash_substractor = int(vector_length * (1/8))\n    hash_length = vector_length - hash_substractor\n    learning_rate_threshold = 0.1\n    learning_rate = 1.0\n    while len(candidate_tuples) < limit:",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_lsh_hash",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def get_lsh_hash(binary_vector, random_indices):\n    sampled_bits = [binary_vector[i] for i in random_indices]\n    hash_value = \"\".join(map(str, sampled_bits))\n    return hash_value\ndef add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "add_to_lsh_table",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def add_to_lsh_table(lsh_table, vector_id, vector, hash_function):\n    bucket_id = get_lsh_hash(vector, hash_function)\n    if bucket_id not in lsh_table:\n        lsh_table[bucket_id] = []\n    if vector_id not in lsh_table[bucket_id]:\n        lsh_table[bucket_id].append(vector_id)\n    return lsh_table\ndef and_or_lsh(encodings, hash_length, iterations):\n    start = time.time()\n    candidates = defaultdict(set)",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "and_or_lsh",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def and_or_lsh(encodings, hash_length, iterations):\n    start = time.time()\n    candidates = defaultdict(set)\n    vector_length = len(next(iter(encodings.values())))\n    for iteration in range(iterations):\n        hash_function = random.sample(range(vector_length), hash_length)\n        lsh_table = defaultdict(list)\n        for index, encoding in encodings.items():\n            sampled_bits = [encoding[i] for i in hash_function]\n            hash_value = tuple(sampled_bits) ",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "faiss_k_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def faiss_k_candidates(vectors_dict, k, index):\n    candidates = {}\n    for vector_id, vector in vectors_dict.items():\n        _, faiss_candidates = index.search(np.array([vector]), k + 1)\n        faiss_candidates = faiss_candidates[0]\n        faiss_candidates = [\n            candidate_id\n            for candidate_id in faiss_candidates\n            if candidate_id != vector_id\n        ]",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "k_faiss_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def k_faiss_hnsw_candidates(vectors_dict, k_nearest_neighbors):\n    faiss_candidates = {}\n    index = faiss.IndexHNSWBinaryB(64, 32)\n    index.train(np.array(list(vectors_dict.values()), dtype=np.float32))\n    index.add(np.array(list(vectors_dict.values()), dtype=np.float32))\n    for k in k_nearest_neighbors:\n        faiss_candidates[k] = faiss_k_candidates(vectors_dict, k, index)\n    return faiss_candidates\ndef faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "faiss_cluster_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def faiss_cluster_candidates(vectors_dict, limit, cluster_labels, index, candidates):\n    candidate_tuples_with_distances = {}\n    fallback_tuples_with_distances = {}\n    for vector_id, vector in vectors_dict.items():\n        faiss_distances, faiss_candidates = index.search(\n            np.array([vector]), len(vectors_dict)\n        )\n        filtered_distances = []\n        filtered_candidates = []\n        for candidate_index, candidate in enumerate(faiss_candidates[0]):",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_hnsw_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def get_hnsw_candidates(\n    encodings, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    dim = 128\n    num_elements = len(encodings)\n    data = np.float32(np.array(list(encodings.values())))\n    ids = np.arange(len(data))\n    index = hnswlib.Index(space = \"l2\", dim = len(data[0]))\n    index.init_index(max_elements = num_elements, ef_construction = 200, M = 16)",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "breadth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def breadth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_matrix = get_bitwise_xor_distance_matrix(vectors_dict)\n    distance_collection_start = time.time()\n    for vector_id, candidate_distances in enumerate(distance_matrix):\n        for candidate_id, distance in enumerate(candidate_distances):\n            if candidate_id == vector_id:\n                continue\n            candidate_tuples_with_distances[",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "depth_bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def depth_bitwise_xor_candidates(vectors_dict, limit, index):\n    candidates = {index: [] for index in vectors_dict.keys()}\n    candidate_tuples_with_distances = {}\n    distance_collection_start = time.time()\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    for vector_id, encoding in vectors_dict.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[vector_id])\n        xor_distances = np.unpackbits(xor_result, axis=1).sum(axis=1)\n        for candidate_id, xor_distance in enumerate(xor_distances):\n            if candidate_id == vector_id:",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "bitwise_xor_candidates",
        "kind": 2,
        "importPath": "gentrain.gentrain.candidate_sourcing",
        "description": "gentrain.gentrain.candidate_sourcing",
        "peekOfCode": "def bitwise_xor_candidates(\n    vectors_dict, limit, search_method=\"depth\", print_execution_time=True\n):\n    start = time.time()\n    first_key = next(iter(vectors_dict))\n    vector_length = len(vectors_dict[first_key])\n    packed_vectors = np.packbits(np.array(list(vectors_dict.values())), axis=1)\n    if search_method == \"breadth\":\n        candidates = breadth_bitwise_xor_candidates(vectors_dict, limit, packed_vectors)\n    else:",
        "detail": "gentrain.gentrain.candidate_sourcing",
        "documentation": {}
    },
    {
        "label": "get_data_aggregate_sample",
        "kind": 2,
        "importPath": "gentrain.gentrain.dataset",
        "description": "gentrain.gentrain.dataset",
        "peekOfCode": "def get_data_aggregate_sample(dataframe, size, seasonal_column, aggregate):\n    aggregate_path = f\"aggregates/{aggregate}\"\n    if not os.path.exists(aggregate_path):\n        os.mkdir(aggregate_path)\n    if not os.path.exists(f\"{aggregate_path}/{size}\"):\n        os.mkdir(f\"{aggregate_path}/{size}\")\n    if os.path.exists(f\"{aggregate_path}/sequences_and_metadata.csv\"):\n        shutil.rmtree(f\"{aggregate_path}/sequences_and_metadata.csv\")\n    n_per_interval = int(size/len(dataframe[seasonal_column].value_counts()))\n    sample = dataframe.groupby(seasonal_column).apply(lambda x: x.sample(n=min(n_per_interval, len(x)), random_state=42), include_groups=False)",
        "detail": "gentrain.gentrain.dataset",
        "documentation": {}
    },
    {
        "label": "prepend_manual",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "def prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r\"^([A-Z])(\\d+)([A-Z])$\", substitution)",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "def get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:\n        match = re.match(r\"^([A-Z])(\\d+)([A-Z])$\", substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        if position not in mutation_positions:\n            mutation_positions[position] = {}\n        mutation_positions[position][\"snp\"] = character\n    for insertion in sequence_mutations[\"insertions\"]:",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "align_samples",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "def align_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1 = np.array([], dtype=str)\n    sequence_2 = np.array([], dtype=str)\n    for current_base_index in range(len(reference_sequence_array) - 1):\n        reference_index_char = reference_sequence_array[current_base_index]\n        additions_1 = np.array([], dtype=str)\n        additions_2 = np.array([], dtype=str)\n        # add remaining reference characters if index is not in positions dictionary\n        if current_base_index not in mutation_positions_1:\n            additions_1 = np.append(additions_1, reference_index_char)",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "calculate_distance",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "def calculate_distance(sequence_1, sequence_2):\n    distance = 0\n    proper_threshold = 5\n    proper_chars_1 = 0\n    proper_chars_2 = 0\n    n_count_1 = np.count_nonzero(sequence_1 == \"N\")\n    n_count_2 = np.count_nonzero(sequence_2 == \"N\")\n    sequence_length_1 = sequence_1.size\n    sequence_length_2 = sequence_2.size\n    active_gap_1 = False",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_distance_for_two_samples",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "def get_distance_for_two_samples(mutation_positions_1, mutation_positions_2):\n    sequence_1, sequence_2 = align_samples(mutation_positions_1, mutation_positions_2)\n    return calculate_distance(sequence_1, sequence_2)",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "ambiguous_chars",
        "kind": 5,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "ambiguous_chars = {\n    \"A\": [\"A\"],\n    \"C\": [\"C\"],\n    \"G\": [\"G\"],\n    \"T\": [\"T\"],\n    \"U\": [\"U\"],\n    \"M\": [\"A\", \"C\"],\n    \"R\": [\"A\", \"G\"],\n    \"S\": [\"C\", \"G\"],\n    \"W\": [\"A\", \"T\"],",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "reference_sequence = \"\"\nfor record in SeqIO.parse((\"../gentrain/gentrain/reference.fasta\"), \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\naligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "aligner",
        "kind": 5,
        "importPath": "gentrain.gentrain.distance_calculation",
        "description": "gentrain.gentrain.distance_calculation",
        "peekOfCode": "aligner = Align.PairwiseAligner(match_score=1.0)\ndef prepend_manual(values, arr):\n    values = np.atleast_1d(values)\n    new_arr = np.empty(len(arr) + len(values), dtype=arr.dtype)\n    new_arr[:len(values)] = values\n    new_arr[len(values):] = arr\n    return new_arr\ndef get_mutation_positions(sequence_mutations):\n    mutation_positions = {}\n    for substitution in sequence_mutations[\"substitutions\"]:",
        "detail": "gentrain.gentrain.distance_calculation",
        "documentation": {}
    },
    {
        "label": "get_bitwise_xor_distance_matrix",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_bitwise_xor_distance_matrix(encodings):\n    start = time.time()\n    hamming_distance_matrix = []\n    packed_vectors = np.packbits(np.array(list(encodings.values())), axis=1)\n    for isolate_id, isolate_encoding in encodings.items():\n        xor_result = np.bitwise_xor(packed_vectors, packed_vectors[isolate_id])\n        hamming_distance_matrix.append(np.unpackbits(xor_result, axis=1).sum(axis=1))\n    hamming_distance_matrix = np.array(hamming_distance_matrix)\n    end = time.time()\n    print(f\"matrix generation time: {round(end - start, 2)}s\")",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_kendall_tau_correlation",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_kendall_tau_correlation(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    correlation, p = kendalltau(matrix_1, matrix_2)\n    return correlation\ndef get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_signed_rmse",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_signed_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse\ndef get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_signed_infection_rmse",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_signed_infection_rmse(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    matrix_1 = matrix_1[infection_mask_1]\n    matrix_2 = matrix_2[infection_mask_1]\n    mean_error = np.mean(matrix_2 - matrix_1)\n    rmse = np.sqrt(np.mean((matrix_2 - matrix_1) ** 2))\n    return np.sign(mean_error) * rmse",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_recall",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_recall(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fn = np.sum(infection_mask_1 & not_infection_mask_2)\n    return tp/(tp+fn)",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_precision",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_precision(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    fp = np.sum(not_infection_mask_1 & infection_mask_2)\n    return tp/(tp+fp)",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_infection_f1",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def get_infection_f1(matrix_1, matrix_2):\n    triu_mask = np.triu(np.ones_like(matrix_1, dtype=bool), k=1)\n    matrix_1 = matrix_1[triu_mask]\n    matrix_2 = matrix_2[triu_mask]\n    not_infection_mask_1 = matrix_1 >= 2\n    infection_mask_1 = matrix_1 < 2\n    not_infection_mask_2 = matrix_2 >= 2\n    infection_mask_2 = matrix_2 < 2\n    tp = np.sum(infection_mask_1 & infection_mask_2)\n    tn = np.sum(not_infection_mask_1 & not_infection_mask_2)",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "median_distance",
        "kind": 2,
        "importPath": "gentrain.gentrain.distance_matrix",
        "description": "gentrain.gentrain.distance_matrix",
        "peekOfCode": "def median_distance(matrix):\n    triu_mask = np.triu(np.ones_like(matrix, dtype=bool), k=1)\n    matrix = matrix[triu_mask]\n    return np.median(matrix)",
        "detail": "gentrain.gentrain.distance_matrix",
        "documentation": {}
    },
    {
        "label": "get_missing_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r\"^(\\d+)(?:-(\\d+))?$\", missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1\n        for position in range(start,end):  \n            if position >= mutations[\"alignmentStart\"] or position <= mutations[\"alignmentEnd\"]:\n                missing_positions.append(position)\n    return missing_positions",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "filter_mutations_by_missings",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def filter_mutations_by_missings(mutations, isolates_df, shifted_relevant_mutations, shifted_reference_positions):\n    mutations[\"alignmentStart\"] = isolates_df[\"alignmentStart\"]\n    mutations[\"alignmentEnd\"] = isolates_df[\"alignmentEnd\"]\n    missing_positions = mutations.apply(lambda x: get_missing_positions(x), axis=1)\n    all_missing_positions = list(itertools.chain.from_iterable(missing_positions))\n    missing_position_appearances = Counter(all_missing_positions)\n    filtered_positions = []\n    for position in range(len(reference_sequence_array)):\n        if missing_position_appearances[position] > (0.05*len(isolates_df)):\n            filtered_positions.append(position)",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "find_and_shift_relevant_mutation_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering):\n    # filter substitions by relevance based on observed behaviours in MST constellations\n    substitution_counts = mutations[\"substitutions\"].explode().value_counts().reset_index()\n    if use_frequency_filtering:\n        substitution_counts = substitution_counts[(substitution_counts[\"count\"]/len(isolates_df) < 0.2) & (substitution_counts[\"count\"] > 1)]\n    relevant_substitution_positions = list(substitution_counts[\"substitutions\"].apply(lambda x: int(re.match(r\"^([A-Z])(\\d+)([A-Z])$\", x).group(2))))\n    shifted_relevant_substitution_positions = [shifted_reference_positions[position] for position in relevant_substitution_positions]\n    if exclude_indels:\n        relevant_substitution_positions.sort()\n        relevant_shifted_substitutions = minified_aligned_sequences[shifted_relevant_substitution_positions]",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_inserted_lengths",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_inserted_lengths(unique_insertion_list):\n    inserted_lengths = {}\n    for insertion in unique_insertion_list:\n        regex_result = re.findall(r\"[0-9]+|:|[A-Za-z]+\", insertion)\n        position = int(regex_result[0])\n        inserted_chars = regex_result[2]\n        if position not in inserted_lengths or position not in inserted_lengths and len(inserted_lengths[position]) < len(inserted_chars):\n            inserted_lengths[position] = 1\n    return dict(sorted(inserted_lengths.items()))\ndef get_shifted_reference_positions(unique_insertion_list):",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_shifted_reference_positions(unique_insertion_list):\n    shifted_reference_positions = {x:x for x in range(1,len(reference_sequence_array)+1)}\n    current_shift = 0\n    inserted_lengths = get_inserted_lengths(unique_insertion_list)\n    for position in shifted_reference_positions:\n        shifted_reference_positions[position] = shifted_reference_positions[position] + current_shift\n        # increment shift for the next position, because 3073:GAA inserts GAA between 3073 and 3074 so that 3074 turns into 3077\n        if position in inserted_lengths.keys():\n            current_shift += inserted_lengths[position]\n    return shifted_reference_positions",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_reference",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_aligned_reference(unique_insertion_list, shifted_reference_positions):\n    aligned_reference = {}\n    aligned_sequences_length = list(shifted_reference_positions.values())[-1]+1\n    for aligned_position in range(1, aligned_sequences_length+1):\n        if aligned_position in list(shifted_reference_positions.values()):\n            reference_position = list(shifted_reference_positions.keys())[list(shifted_reference_positions.values()).index(aligned_position)]\n            aligned_reference[aligned_position] = reference_sequence[reference_position - 1]\n        else:\n            aligned_reference[aligned_position] = \"-\"\n    return aligned_reference",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_mutation_dict",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = {i: 0 for i in range(1, len(aligned_reference)+1)}\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r\"^([A-Z])(\\d+)([A-Z])$\", substitution)\n        position = int(match.group(2))\n        aligned_sequence[shifted_reference_positions[position]] = 1\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:\n        match = re.match(r\"^(\\d+):([A-Z]+)$\", insertion)",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_aligned_nucleotide_dict",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels=False):\n    aligned_sequence = aligned_reference.copy()\n    for substitution in row[\"substitutions\"]:\n        match = re.match(r\"^([A-Z])(\\d+)([A-Z])$\", substitution)\n        position = int(match.group(2))\n        character = match.group(3)\n        aligned_sequence[shifted_reference_positions[position]] = character\n    if exclude_indels:\n        return aligned_sequence\n    for insertion in row[\"insertions\"]:",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_shifted_reference_positions_from_csv",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_shifted_reference_positions_from_csv(isolates_df):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    return shifted_reference_positions\ndef filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "filter_and_align_mutations",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def filter_and_align_mutations(isolates_df, mutation_sensitive, exclude_indels, use_frequency_filtering, filter_N):\n    mutations = get_mutations_from_dataframe(isolates_df)\n    unique_insertion_list = list(set(mutations[\"insertions\"].explode().dropna().tolist()))\n    shifted_reference_positions = get_shifted_reference_positions(unique_insertion_list)\n    aligned_reference = get_aligned_reference(unique_insertion_list, shifted_reference_positions)\n    aligned_sequences_dict = list(mutations.apply(lambda row: get_aligned_mutation_dict(row, aligned_reference, shifted_reference_positions, exclude_indels) if mutation_sensitive else get_aligned_nucleotide_dict(row, aligned_reference, shifted_reference_positions, exclude_indels), axis=1))\n    aligned_sequences_df = pd.DataFrame(aligned_sequences_dict)\n    minified_aligned_sequences = aligned_sequences_df\n    shifted_relevant_mutations, shifted_reference_positions = find_and_shift_relevant_mutation_positions(mutations, isolates_df, minified_aligned_sequences, shifted_reference_positions, exclude_indels, use_frequency_filtering)\n    if filter_N:",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "trim_vocab_for_faiss",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def trim_vocab_for_faiss(mutations, m):\n    columns_to_drop = len(mutations.columns) % m\n    cols = []\n    if columns_to_drop > 0:\n        for col in mutations.columns:\n            value_counts = mutations[col].value_counts(dropna=False)\n            if len(value_counts) == 2 and (1 in value_counts.values or 2 in value_counts.values):\n                cols.append(col)\n                columns_to_drop = columns_to_drop - 1\n            if columns_to_drop == 0:",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_mutation_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_mutation_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, True, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    end = time.time()",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_nucleotide_sensitive_encodings",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def get_nucleotide_sensitive_encodings(isolates_df, exclude_indels = False, use_frequency_filtering = True, filter_N = True):\n    start = time.time()\n    filtered_and_aligned_mutations = filter_and_align_mutations(isolates_df, False, exclude_indels, use_frequency_filtering, filter_N)\n    mutations = trim_vocab_for_faiss(filtered_and_aligned_mutations, 8)\n    char_array = mutations.to_numpy()\n    unique_chars = np.unique(char_array)\n    char_to_int = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n    encode = np.vectorize(lambda x: float(char_to_int[x]))\n    encodings = encode(char_array)\n    one_hot_encodings = generate_one_hot_encoding(encodings)    ",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "generate_one_hot_encoding",
        "kind": 2,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "def generate_one_hot_encoding(encodings):\n    unique_values = sorted(set(val for encoding in encodings for val in encoding))\n    value_to_index = {val: idx for idx, val in enumerate(unique_values)}\n    def one_hot_encode(encoding, value_to_index):\n        num_classes = len(value_to_index)\n        return np.eye(num_classes, dtype=\"float32\")[[value_to_index[val] for val in encoding]]\n    return [one_hot_encode(encoding, value_to_index).flatten() for encoding in encodings]",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_sequence",
        "kind": 5,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "reference_sequence = \"\"\nreference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r\"^(\\d+)(?:-(\\d+))?$\", missing)",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r\"^(\\d+)(?:-(\\d+))?$\", missing)\n        start = int(match.group(1))",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.gentrain.encoding",
        "description": "gentrain.gentrain.encoding",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\n    reference_sequence_array = np.array(list(reference_sequence), dtype=str)\ndef get_missing_positions(mutations):\n    missing_positions = []\n    for missing in mutations[\"missing\"]:\n        match = re.match(r\"^(\\d+)(?:-(\\d+))?$\", missing)\n        start = int(match.group(1))\n        end = int(match.group(2)) + 1 if match.group(2) else start + 1",
        "detail": "gentrain.gentrain.encoding",
        "documentation": {}
    },
    {
        "label": "get_infection_rate",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_infection_rate(distance_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    filtered = distance_matrix_df.where(tri_mask).to_numpy()\n    infections_count = (filtered <= distance_threshold).sum().sum()\n    total_distances_count = np.count_nonzero(~np.isnan(filtered))\n    return infections_count/total_distances_count\ndef get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_infection_chain_participation_rate",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_infection_chain_participation_rate(distance_matrix, distance_threshold = 1):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    mask_off_diagonal = ~np.eye(distance_matrix_df.shape[0], dtype=bool)\n    distance_matrix_df = distance_matrix_df.where(mask_off_diagonal)\n    condition = (distance_matrix_df < distance_threshold).any(axis=1)\n    return condition.sum()/len(distance_matrix_df)\ndef get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_infection_detection_scores",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_infection_detection_scores(complete_matrix, observed_matrix, distance_threshold = 1):\n    tri_mask = np.triu(np.ones(complete_matrix.shape), k=1).astype(bool)\n    complete_matrix_df = pd.DataFrame(complete_matrix)\n    filtered_complete_matrix = complete_matrix_df.where(tri_mask).to_numpy()\n    observed_matrix_df = pd.DataFrame(observed_matrix)\n    filtered_observed_matrix = observed_matrix_df.where(tri_mask).to_numpy()\n    tp_mask = (filtered_observed_matrix <= distance_threshold) & (filtered_complete_matrix <= distance_threshold)\n    tp = len(np.argwhere(tp_mask))\n    tn_mask = (filtered_observed_matrix > distance_threshold) & (filtered_complete_matrix > distance_threshold)\n    tn = len(np.argwhere(tn_mask))",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_lineage_purity",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_lineage_purity(lineages, communities):\n    df = pd.DataFrame({\n        \"community\": communities,\n        \"lineage\":   lineages\n    })\n    counts = df.groupby([\"community\", \"lineage\"]) \\\n               .size() \\\n               .rename(\"count\")\n    community_sizes = counts.groupby(level=0) \\\n                            .sum() \\",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "candidate_evaluation_and_matrices",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def candidate_evaluation_and_matrices(candidates, distance_matrix, runtime):\n    distance_matrix_df = pd.DataFrame(distance_matrix)\n    tri_mask = np.triu(np.ones(distance_matrix.shape), k=1).astype(bool)\n    filtered = distance_matrix_df.where(tri_mask)\n    infections_count = (filtered < 2).sum().sum()\n    total_distances_count = filtered.count().sum()\n    mask = np.zeros_like(distance_matrix, dtype=bool)\n    for i, candidates in candidates.items():\n        for j in candidates:\n            mask[i, j] = True",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_community_map",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_community_map(partition):\n    community_map = {}\n    for sample_id, community_id in partition.items():\n        if community_id not in community_map:\n            community_map[community_id] = []\n        community_map[community_id].append(sample_id)\n    return community_map\ndef get_communities_larger_than(community_map, min_size):\n    return {\n        k: v",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_communities_larger_than",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_communities_larger_than(community_map, min_size):\n    return {\n        k: v\n        for k, v in community_map.items()\n        if isinstance(v, list) and len(v) > min_size\n    }\ndef get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_community_labels_for_sample_ids",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_community_labels_for_sample_ids(community_labels, ids):\n    return [community_labels[i] for i in ids]\ndef candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "candidate_graph_evaluation",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def candidate_graph_evaluation(candidate_mst, complete_mst, candidate_community_labels, complete_community_labels, lineages):\n    return {\n        \"mean_edge_weight\": mean_edge_weight(candidate_mst),\n        \"mean_edge_weight_diff\": mean_edge_weight(candidate_mst) - mean_edge_weight(complete_mst),\n        \"max_edge_weight\": max_edge_weight(candidate_mst),\n        \"subgraph_count\": len(list(nx.connected_components(candidate_mst))),\n        \"adjusted_rand_index\": adjusted_rand_score(\n            complete_community_labels, candidate_community_labels\n        ),\n        \"lineage_purity\": get_lineage_purity(lineages, candidate_community_labels),",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_candidate_evaluation_and_export_mst",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_candidate_evaluation_and_export_mst(\n    method_name,\n    candidates,\n    graph_path,\n    distance_matrix,\n    complete_community_labels,\n    complete_mst,\n    lineages,\n    isolates_df,\n    runtime,",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_computation_rate_plot",
        "kind": 2,
        "importPath": "gentrain.gentrain.evaluation",
        "description": "gentrain.gentrain.evaluation",
        "peekOfCode": "def get_computation_rate_plot(field, evaluation_collection, y_axis_title, legend=dict(\n            x=0.55,\n            y=0.1,\n            xanchor=\"left\",\n            yanchor=\"bottom\",\n            font=dict(size=35),\n        ), computation_rates=[0.05,0.1,0.15,0.2]):\n    fig = go.Figure()\n    for index, method_evaluation in enumerate(evaluation_collection.items()):\n        method = method_evaluation[0]",
        "detail": "gentrain.gentrain.evaluation",
        "documentation": {}
    },
    {
        "label": "get_outbreak_community_labels",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def get_outbreak_community_labels(graph, distance_threshold = None):\n    graph_copy = graph.copy()\n    if distance_threshold:\n        edges_to_remove = [(u, v) for u, v, d in graph_copy.edges(data=True) if d[\"weight\"] > distance_threshold]\n        graph_copy.remove_edges_from(edges_to_remove)\n    partition = community_louvain.best_partition(graph_copy, random_state=42)\n    community_labels = list(partition.values())\n    return community_labels\ndef get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_connected_component_labels",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def get_connected_component_labels(mst, distance_threshold = 1):\n    mst_copy = mst.copy()\n    edges_to_remove = [(u, v) for u, v, d in mst_copy.edges(data=True) if d[\"weight\"] > distance_threshold]\n    mst_copy.remove_edges_from(edges_to_remove)\n    communities = list(nx.connected_components(mst_copy))\n    outbreak_communities = sorted(communities, key=len, reverse=True)\n    community_labels = {}\n    for community_id, community_nodes in enumerate(outbreak_communities):\n        for node_id in community_nodes:\n            community_labels[node_id] = community_id",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "mean_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def mean_edge_weight(G):\n    total_weight = sum(data.get(\"weight\") for u, v, data in G.edges(data=True))\n    num_edges = G.number_of_edges()\n    return total_weight / num_edges if num_edges > 0 else 0.0\ndef median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "median_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def median_edge_weight(G):\n    weights = [data.get(\"weight\") for u, v, data in G.edges(data=True)]\n    if not weights:\n        return 0.0\n    return np.median(weights)\ndef max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "max_edge_weight",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def max_edge_weight(G):\n    return max(data.get(\"weight\") for u, v, data in G.edges(data=True))\ndef build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_mst",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def build_mst(graph):\n    start = time.time()\n    mst = nx.minimum_spanning_tree(graph, algorithm=\"prim\")\n    for _, _, data in mst.edges(data=True):\n        # since gephi can't deal with weights of 0, we set 0 weights to 0.1\n        # this does not effect resulting graphs since all other weights are interger values\n        # and 0.1 remaind the smallest present edge weight\n        if data.get(\"weight\", 1) == 0:\n            data[\"weight\"] = 0.1   \n    end = time.time()",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "build_graph",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def build_graph(distance_matrix):\n    graph = nx.Graph()\n    n = distance_matrix.shape[0]\n    graph.add_nodes_from(range(n))\n    for i in range(n):\n        for j in range(i+1, n):\n            distance = distance_matrix[i][j]\n            if not np.isnan(distance):\n                label = \"regular\"\n                if distance < 2:",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "export_graph_gexf",
        "kind": 2,
        "importPath": "gentrain.gentrain.graph",
        "description": "gentrain.gentrain.graph",
        "peekOfCode": "def export_graph_gexf(graph, community_labels, dataset, name):\n    datetime_sampling_dates = pd.to_datetime(dataset[\"date_of_sampling\"])\n    numeric_dates = (datetime_sampling_dates - datetime_sampling_dates.min()).dt.days\n    nx.set_node_attributes(graph, {node: community_labels[int(node)] for node in graph.nodes()}, name=\"community\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"clade\"] for node in graph.nodes()}, name=\"clade\")            \n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"Nextclade_pango\"] for node in graph.nodes()}, name=\"pango\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.city\"] for node in graph.nodes()}, name=\"city\")\n    nx.set_node_attributes(graph, {node: dataset.iloc[int(node)][\"prime_diagnostic_lab.state\"] for node in graph.nodes()}, name=\"state\")\n    nx.set_node_attributes(graph, {node: numeric_dates.iloc[int(node)] for node in graph.nodes()}, name=\"sampling_date\")\n    nx.write_gexf(graph, f\"{name}.gexf\")",
        "detail": "gentrain.gentrain.graph",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_dataframe",
        "kind": 2,
        "importPath": "gentrain.gentrain.nextclade",
        "description": "gentrain.gentrain.nextclade",
        "peekOfCode": "def get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "get_mutations_from_csv",
        "kind": 2,
        "importPath": "gentrain.gentrain.nextclade",
        "description": "gentrain.gentrain.nextclade",
        "peekOfCode": "def get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)\n    return get_mutations_from_dataframe(dataframe)",
        "detail": "gentrain.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "reference_fasta_bytes",
        "kind": 5,
        "importPath": "gentrain.gentrain.nextclade",
        "description": "gentrain.gentrain.nextclade",
        "peekOfCode": "reference_fasta_bytes = pkgutil.get_data(__package__, \"reference.fasta\")\nreference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):",
        "detail": "gentrain.gentrain.nextclade",
        "documentation": {}
    },
    {
        "label": "reference_fasta_io",
        "kind": 5,
        "importPath": "gentrain.gentrain.nextclade",
        "description": "gentrain.gentrain.nextclade",
        "peekOfCode": "reference_fasta_io = StringIO(reference_fasta_bytes.decode())\nfor record in SeqIO.parse(reference_fasta_io, \"fasta\"):\n    reference_sequence = str(record.seq)\ndef get_mutations_from_dataframe(dataframe):\n    mutations = dataframe[\n        [\"substitutions\", \"insertions\", \"deletions\", \"missing\", \"nonACGTNs\"]]\n    mutations = mutations.apply(lambda col: col.map(lambda x: x.split(\",\") if isinstance(x, str) else []))\n    return mutations\ndef get_mutations_from_csv(csv_path):\n    dataframe = pd.read_csv(csv_path, delimiter=\";\", low_memory=False)",
        "detail": "gentrain.gentrain.nextclade",
        "documentation": {}
    }
]